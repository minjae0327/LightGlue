{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Image Matching and Homography Estimation with OpenCV and LightGlue"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import cv2 \n","import time\n","import json\n","import math\n","import copy\n","import torch\n","import numpy as np\n","from vidstab import VidStab\n","import matplotlib.pyplot as plt\n","\n","from lightglue import viz2d\n","from lightglue import LightGlue, SuperPoint, DISK\n","from lightglue.utils import load_image, rbd, load_image_from_path\n","import CSRansac\n","\n","from vidstab import VidStab"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["os.environ['KMP_DUPLICATE_LIB_OK']='True'"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")  # 'mps', 'cpu'\n","\n","extractor = SuperPoint(max_num_keypoints=2048).eval().to(device)  # load the extractor\n","#matcher = LightGlue(features='superpoint', depth_confidence=0.9, width_confidence=0.95).eval().to(device)\n","matcher = LightGlue(features='superpoint', depth_confidence=-1, width_confidence=-1).eval().to(device)\n","#matcher.compile(mode='reduce-overhead')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.2.0\n","mps\n"]}],"source":["print(torch.__version__)\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LightglueHomography:\n","    def __init__(self):\n","        self.stabilizer = VidStab()\n","        self.origin_coordinate = []\n","        self.float_origin_coordinate = []\n","        self.aircraft_datasets = \"datasets\"\n","        self.video_dir = os.path.join(self.aircraft_datasets, \"video\")\n","        self.target_image_dir = os.path.join(self.aircraft_datasets, \"target_image\")\n","        self.lables = os.path.join(self.aircraft_datasets + \"/label\")\n","        \n","        # 에러 평가 변수\n","        self.disappear_errors = []\n","        self.misannotate_errors = []\n","        self.pixel_errors = []\n","        self.missing_inlier = 0\n","        self.failed_inliers = 0\n","        \n","        \n","    def __call__(self):\n","        pass\n","        \n","        \n","    def set_coord(self):\n","        # 원점 좌표값 불러오기\n","        for label_file in os.listdir(self.lables):\n","            self.label_path = os.path.join(self.lables, label_file)\n","            with open(self.label_path, \"r\") as f:\n","                json_file = json.load(f)\n","                coord = json_file[\"targetAnnotation\"]\n","                float_coord = copy.deepcopy(coord)\n","                self.float_origin_coordinate.append(float_coord)\n","                \n","                coord[0] = round(coord[0] * 640, 4)\n","                coord[1] = round(coord[1] * 480, 4)\n","                self.origin_coordinate.append(coord)\n","            \n","                \n","    def get_target_images(self):\n","        # 원본 이미지를 가져오는 코드\n","        target_images = []\n","        for image_file in os.listdir(self.target_image_dir):\n","            image_path = os.path.join(self.target_image_dir, image_file)\n","            target_images.append(image_path)\n","            \n","        return target_images\n","    \n","    \n","    def experiment(self):\n","        # 매칭되는 호모그래피를 구하여 원점을 투영 변환한 후, 에러를 측정\n","        # 에러 측정을 위해 총 10번 반복\n","        target_images = self.get_target_images()\n","        len_coord = len(self.origin_coordinate)\n","        \n","        for i in range(10):\n","            for i in range(len_coord):\n","                target_image = target_images[i]\n","                len_target_image = len(target_images)\n","                \n","                x = self.origin_coordinate[i][0]\n","                y = self.origin_coordinate[i][1]\n","                \n","                target_image = load_image(target_image, grayscale=True)\n","                \n","                # vide_dir에 있는 모든 비디오를 가져옴\n","                for video_file in os.listdir(self.video_dir):\n","                    video_path = os.path.join(self.video_dir, video_file)\n","                    cap = cv2.VideoCapture(video_path)\n","                    \n","                    # 각 프레임 처리, 에러처리도 동시에 진행\n","                    while True:\n","                        ret, frame = cap.read()\n","                        if not ret:\n","                            break\n","                        \n","                        # 특징점 매칭\n","                        results = self.matching_keypoints(target_image, frame)\n","                        target_keypoint = results[\"points0\"].cpu().numpy()\n","                        frame_keypoint = results[\"points1\"].cpu().numpy()\n","                        \n","                    # 호모그래피 추정\n","                        homography, mask = CSRansac.csransac(target_keypoint, frame_keypoint))\n","                        projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","                        \n","                        # 에러 측정\n","                        self.get_errors(target_keypoint, mask, projected_pts, x, y)\n","                            \n","                    cap.release()\n","    \n","    \n","    def matching_keypoints(self, target_img, video_img, stabilizing=False):\n","        # 이미지를 불러옴\n","        img0 = load_image(target_img, grayscale=True)\n","        if stabilizing == True:\n","            img1 = self.stabilizer.stabilize_frame(video_img)\n","            img1 = load_image(img1, grayscale=True)\n","        else:\n","            img1 = load_image(video_img , grayscale=True)\n","\n","        # extract local features\n","        feats0 = extractor.extract(img0.to(device))  # auto-resize the image, disable with resize=None\n","        feats1 = extractor.extract(img1.to(device))\n","\n","        # match the features\n","        matches01 = matcher({'image0': feats0, 'image1': feats1})\n","        feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]  # remove batch dimension\n","\n","        # get results\n","        kpts0 = feats0[\"keypoints\"]\n","        kpts1 = feats1[\"keypoints\"]\n","        matches = matches01['matches']  # indices with shape (K,2)\n","        points0 = kpts0[matches[..., 0]]  # coordinates in img0, shape (K,2)\n","        points1 = kpts1[matches[..., 1]]  # coordinates in img1, shape (K,2)\n","\n","        return {\n","            \"points0\": points0,\n","            \"points1\": points1,\n","        }\n","    \n","    \n","    #에러 측정 \n","    def measure_errors(self, keypoint, mask, coord, x, y):\n","        if len(keypoint) < 6:\n","            self.missing_inlier += 1\n","        \n","        if mask == 0.3:\n","            self.failed_inliers += 1\n","        \n","        x = x / 640\n","        y = y / 480\n","        _x = coord[0] / 640\n","        _y = coord[1] / 480\n","        \n","        if _x < 0 or _x > 1 or _y < 0 or _y > 1:\n","            self.disappear_error += 1\n","        \n","        distance = math.sqrt((x - _x)**2 + (y - _y)**2)\n","        \n","        if distance > 0.1:\n","            self.misannotate_error += 1\n","            \n","        if distance > self.pixel_error:\n","            self.pixel_error = distance\n","     \n","            \n","    def get_errors(self):\n","        return {\n","            \"disappear_error\": self.disappear_error,\n","            \"misannotate_error\": self.misannotate_error,\n","            \"pixel_error\": self.pixel_error,\n","            \"missing_inlier\": self.missing_inlier,\n","            \"failed_inliers\": self.failed_inliers\n","        }"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["disappear_error: 279.5\n","num_error: 867.2\n","pixel_error: 10.09973137927756\n"]}],"source":["error1 = sum(disappear_errors) / len(disappear_errors)\n","error2 = sum(misannotate_errors) / len(misannotate_errors)\n","error3 = sum(pixel_errors) / len(pixel_errors)\n","\n","print(\"disappear_error:\", error1)\n","print(\"num_error:\", error2)\n","print(\"pixel_error:\", error3)\n","print(\"missing_inlier:\", missing_inlier)\n","print(\"failed_inliers:\", failed_inliers)   "]},{"cell_type":"markdown","metadata":{},"source":["## check speed"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m image1 \u001b[38;5;241m=\u001b[39m load_image(frame, grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m feats1 \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mextract(image1\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 29\u001b[0m matches01 \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m feats0, feats1, matches01 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m     rbd(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [feats0, feats1, matches01]\n\u001b[1;32m     33\u001b[0m ]  \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n\u001b[1;32m     35\u001b[0m kpts0, kpts1, matches \u001b[38;5;241m=\u001b[39m feats0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], feats1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], matches01[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m~/anaconda3/envs/lightglue/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/lightglue/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/GitHub/LightGlue/lightglue/lightglue.py:471\u001b[0m, in \u001b[0;36mLightGlue.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03mMatch keypoints and descriptors between two images\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    matches: List[[Si x 2]], scores: List[[Si]]\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mmp, device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/GitHub/LightGlue/lightglue/lightglue.py:581\u001b[0m, in \u001b[0;36mLightGlue._forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches0\u001b[39m\u001b[38;5;124m\"\u001b[39m: m0,\n\u001b[1;32m    570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches1\u001b[39m\u001b[38;5;124m\"\u001b[39m: m1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprune1\u001b[39m\u001b[38;5;124m\"\u001b[39m: prune1,\n\u001b[1;32m    578\u001b[0m         }\n\u001b[1;32m    580\u001b[0m desc0, desc1 \u001b[38;5;241m=\u001b[39m desc0[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :m, :], desc1[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n, :]  \u001b[38;5;66;03m# remove padding\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m scores, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_assignment\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m m0, m1, mscores0, mscores1 \u001b[38;5;241m=\u001b[39m filter_matches(scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mfilter_threshold)\n\u001b[1;32m    583\u001b[0m matches, mscores \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/lightglue/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/lightglue/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/GitHub/LightGlue/lightglue/lightglue.py:286\u001b[0m, in \u001b[0;36mMatchAssignment.forward\u001b[0;34m(self, desc0, desc1)\u001b[0m\n\u001b[1;32m    284\u001b[0m _, _, d \u001b[38;5;241m=\u001b[39m mdesc0\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    285\u001b[0m mdesc0, mdesc1 \u001b[38;5;241m=\u001b[39m mdesc0 \u001b[38;5;241m/\u001b[39m d\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.25\u001b[39m, mdesc1 \u001b[38;5;241m/\u001b[39m d\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m--> 286\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbmd,bnd->bmn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdesc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdesc1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m z0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatchability(desc0)\n\u001b[1;32m    288\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatchability(desc1)\n","File \u001b[0;32m~/anaconda3/envs/lightglue/lib/python3.12/site-packages/torch/functional.py:193\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# Overwriting reason:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# call here.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msplit(split_size_or_sections, dim)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"einsum(equation, *operands) -> Tensor\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m                [ 0.3311,  5.5201, -3.0356]])\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_einsum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopt_einsum\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n","\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n","\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n","\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."]}],"source":["# video_frames 폴더에서 프레임 파일 리스트 가져오기\n","video_frames = os.listdir('video')\n","\n","# 프레임 수 초기화\n","frame_count = 0\n","\n","# 프레임 별 처리 시간 리스트 초기화\n","frame_processing_times = []\n","\n","x = 637 // 2\n","y = 367 // 2\n","\n","image0 = load_image_from_path(\"img1.png\", grayscale=True)\n","cap = cv2.VideoCapture('demo_video_resized.mp4')\n","\n","# 각 프레임 처리\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    start_time = time.time()\n","    \n","    feats0 = extractor.extract(image0.to(device))\n","    image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(frame, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    \n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, _ = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","    \n","    # 현재 시간 측정\n","    current_time = time.time()\n","    \n","    # 프레임 처리 시간 계산\n","    frame_processing_time = current_time - start_time\n","    frame_processing_times.append(frame_processing_time)\n","    \n","    # 이전 프레임 처리 시간 업데이트\n","    prev_frame_time = current_time\n","\n","    # FPS 계산\n","    fps = 1.0 / frame_processing_time\n","\n","    # 프레임 수 증가\n","    frame_count += 1\n","\n","    \n","    cv2.imshow('frame', frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","    \n","    \n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# 전체 처리 시간 계산\n","total_processing_time = sum(frame_processing_times)\n","\n","# 전체 프레임 수와 전체 처리 시간을 사용하여 평균 FPS 계산\n","average_fps = frame_count / total_processing_time\n","\n","print(f\"Total Frames Processed: {frame_count}\")\n","print(f\"Average FPS: {average_fps:.2f}\")"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["from vidstab import VidStab\n","\n","# Using defaults\n","stabilizer = VidStab()\n","stabilizer.stabilize(input_path='demo_video_resized.mp4', output_path='stable_demo_video.mp4')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["disappear_error: 0\n","misannotate_error: 334\n","pixel_error: 0.14165977565377297\n"]}],"source":["cap = cv2.VideoCapture('datasets/unstable_version.mp4')\n","\n","x = 319\n","y = 238\n","\n","disappear_error = 0\n","misannotate_error = 0\n","pixel_error = 0\n","\n","image0 = load_image(\"datasets/origin.png\", grayscale=True)\n","\n","fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n","out = cv2.VideoWriter('before_stabilize.avi', fourcc, 30, (640, 480))\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    feats0 = extractor.extract(image0.to(device))\n","    #image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(frame, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    \n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, mask = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    if mask < 0.3:\n","        cv2.imshow('frame', frame)\n","        continue\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    _x = projected_pts[0] / 640\n","    _y = projected_pts[1] / 480\n","    \n","    # 에러 측정\n","    if _x < 0 or _x > 1 or _y < 0 or _y > 1:\n","        disappear_error += 1\n","        \n","    distance = math.sqrt((x / 640 - _x)**2 + (y / 640 - _y)**2)\n","    \n","    if distance > 0.1:\n","        misannotate_error += 1\n","        \n","    if distance > pixel_error:\n","        pixel_error = distance\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","    \n","    cv2.imshow('frame', frame)\n","    \n","    out.write(frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()\n","\n","print(\"disappear_error:\", disappear_error)\n","print(\"misannotate_error:\", misannotate_error)\n","print(\"pixel_error:\", pixel_error)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"ename":"error","evalue":"OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:124: error: (-215:Assertion failed) bmi && width >= 0 && height >= 0 && (bpp == 8 || bpp == 24 || bpp == 32) in function 'FillBitmapInfo'\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[1;32mIn[39], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m     pixel_error \u001b[38;5;241m=\u001b[39m distance\n\u001b[0;32m     55\u001b[0m cv2\u001b[38;5;241m.\u001b[39mcircle(frame, (\u001b[38;5;28mint\u001b[39m(projected_pts[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(projected_pts[\u001b[38;5;241m1\u001b[39m])), \u001b[38;5;241m5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m out\u001b[38;5;241m.\u001b[39mwrite(frame)\n\u001b[0;32m     61\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m5\u001b[39m)\n","\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:124: error: (-215:Assertion failed) bmi && width >= 0 && height >= 0 && (bpp == 8 || bpp == 24 || bpp == 32) in function 'FillBitmapInfo'\n"]}],"source":["cap = cv2.VideoCapture('datasets/unstable_version.mp4')\n","\n","x = 319\n","y = 238\n","\n","disappear_error = 0\n","misannotate_error = 0\n","pixel_error = 0\n","\n","image0 = load_image(\"datasets/origin.png\", grayscale=True)\n","\n","fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n","out = cv2.VideoWriter('after_stabilize.avi', fourcc, 30, (640, 480))\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    feats0 = extractor.extract(image0.to(device))\n","    image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(image1, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    \n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, mask = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    if mask < 0.3:\n","        cv2.imshow('frame', frame)\n","        continue\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    _x = projected_pts[0] / 640\n","    _y = projected_pts[1] / 480\n","    \n","    # 에러 측정\n","    if _x < 0 or _x > 1 or _y < 0 or _y > 1:\n","        disappear_error += 1\n","        \n","    distance = math.sqrt((x / 640 - _x)**2 + (y / 640 - _y)**2)\n","    \n","    if distance > 0.1:\n","        misannotate_error += 1\n","        \n","    if distance > pixel_error:\n","        pixel_error = distance\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","    \n","    cv2.imshow('frame', frame)\n","    \n","    out.write(frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()\n","\n","print(\"disappear_error:\", disappear_error)\n","print(\"misannotate_error:\", misannotate_error)\n","print(\"pixel_error:\", pixel_error)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":4}
