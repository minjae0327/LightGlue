{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Image Matching and Homography Estimation with OpenCV and LightGlue"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import cv2 \n","import time\n","import json\n","import math\n","import copy\n","import torch\n","import numpy as np\n","from vidstab import VidStab\n","import matplotlib.pyplot as plt\n","\n","from lightglue import viz2d\n","from lightglue import LightGlue, SuperPoint, DISK\n","from lightglue.utils import load_image, rbd, load_image_from_path\n","import CSRansac\n","\n","from vidstab import VidStab"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["os.environ['KMP_DUPLICATE_LIB_OK']='True'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 'mps', 'cpu'\n","\n","extractor = SuperPoint(max_num_keypoints=2048).eval().to(device)  # load the extractor\n","#matcher = LightGlue(features='superpoint', depth_confidence=0.9, width_confidence=0.95).eval().to(device)\n","matcher = LightGlue(features='superpoint', depth_confidence=-1, width_confidence=-1).eval().to(device)\n","#matcher.compile(mode='reduce-overhead')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.1.2\n","cuda\n"]}],"source":["print(torch.__version__)\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset 전처리"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["aircraft_datasets = \"datasets\"\n","\n","lables = os.path.join(aircraft_datasets + \"/label\")\n","video_dir = os.path.join(aircraft_datasets, \"video\")\n","target_image_dir = os.path.join(aircraft_datasets, \"target_image\")\n","# 비디오 안정화 객체 생성\n","stabilizer = VidStab()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","\n","# 저장할 동영상 파일명 및 코덱 설정\n","output_video_path = 'output_video.mp4'\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","output_video_fps = 30.0  # 저장할 동영상의 프레임 속도\n","\n","cap = cv2.VideoCapture('demo_video_resized.mp4')\n","image0 = load_image(\"img1.png\", grayscale=True)\n","\n","# 저장할 동영상의 너비와 높이 설정\n","frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# VideoWriter 객체 생성\n","out = cv2.VideoWriter(output_video_path, fourcc, output_video_fps, (frame_width, frame_height))\n","\n","count = 0\n","\n","# 각 프레임을 VideoWriter에 쓰기\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # 프레임에 작업 수행\n","    if count == 0:\n","        count += 1\n","        cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","        cv2.imshow('frame', frame)\n","        continue\n","    feats0 = extractor.extract(image0.to(device))\n","    image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(frame, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, _ = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","\n","    # 동영상 파일에 프레임 추가\n","    out.write(frame)\n","\n","    # 화면에 표시\n","    cv2.imshow('frame', frame)\n","    \n","    # 종료 키 입력 확인\n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","\n","# VideoWriter 객체 해제\n","out.release()\n","\n","# VideoCapture 객체 해제\n","cap.release()\n","\n","# 모든 창 닫기\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Frames Processed: 367\n","Average FPS: 11.41\n"]}],"source":["# video_frames 폴더에서 프레임 파일 리스트 가져오기\n","video_frames = os.listdir('video')\n","\n","# 프레임 수 초기화\n","frame_count = 0\n","\n","# 프레임 별 처리 시간 리스트 초기화\n","frame_processing_times = []\n","\n","image0 = load_image(\"img1.png\", grayscale=True)\n","x = 637 // 2\n","y = 367 // 2\n","count = 0\n","\n","cap = cv2.VideoCapture('demo_video_resized.mp4')\n","\n","# 각 프레임 처리\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    start_time = time.time()\n","    \n","    if count == 0:\n","        count += 1\n","        cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","        cv2.imshow('frame', frame)\n","        continue\n","    feats0 = extractor.extract(image0.to(device))\n","    image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(frame, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, _ = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","    \n","    # 현재 시간 측정\n","    current_time = time.time()\n","    \n","    # 프레임 처리 시간 계산\n","    frame_processing_time = current_time - start_time\n","    frame_processing_times.append(frame_processing_time)\n","    \n","    # 이전 프레임 처리 시간 업데이트\n","    prev_frame_time = current_time\n","\n","    # FPS 계산\n","    fps = 1.0 / frame_processing_time\n","\n","    # 프레임 수 증가\n","    frame_count += 1\n","\n","    image0 = image1\n","    cv2.imshow('frame', frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","    \n","    \n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# 전체 처리 시간 계산\n","total_processing_time = sum(frame_processing_times)\n","\n","# 전체 프레임 수와 전체 처리 시간을 사용하여 평균 FPS 계산\n","average_fps = frame_count / total_processing_time\n","\n","print(f\"Total Frames Processed: {frame_count}\")\n","print(f\"Average FPS: {average_fps:.2f}\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# video_frames 폴더에서 프레임 파일 리스트 가져오기\n","video_frames = os.listdir('video')\n","cap = cv2.VideoCapture('demo_video_resized.mp4')\n","\n","image0 = load_image(\"img1.png\", grayscale=True)\n","x = 637 // 2\n","y = 367 // 2\n","count = 0\n","\n","# 각 프레임 처리\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    start_time = time.time()\n","    if count == 0:\n","        count += 1\n","        cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","        cv2.imshow('frame', frame)\n","        continue\n","    feats0 = extractor.extract(image0.to(device))\n","    image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(frame, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    \n","    #print(feats0[\"keypoints\"].shape, feats1[\"keypoints\"].shape)\n","    \n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, mask = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    image0 = image1\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","\n","    cv2.imshow('frame', frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","    \n","    \n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# 전체 처리 시간 계산"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[319.172, 270.5525], [320.0, 265.2454], [344.4649, 256.0291], [313.5761, 257.2958], [325.4817, 168.0838], [315.9396, 202.4891], [325.4792, 168.0804], [312.3912, 306.4268], [320.0, 265.2386], [331.4872, 26.9028], [316.5232, 203.0878], [329.4775, 59.023], [320.0, 337.5758], [324.1364, 161.3599], [309.3466, 253.7444], [321.2631, 248.8727], [332.8524, 236.0226], [326.0481, 203.8017], [318.4895, 251.0605], [320.9647, 255.8256], [321.2552, 215.7061], [319.4533, 225.7516], [319.4534, 180.869], [321.2005, 215.6378], [321.2277, 215.6717], [316.3752, 230.084], [316.2056, 231.4328], [320.8982, 312.2862], [320.9509, 198.6214], [315.9281, 231.4998], [320.8952, 257.6141], [320.8216, 257.4771], [320.8206, 257.478], [320.6533, 290.0106], [320.7295, 257.2927], [320.0, 291.9199], [320.0, 257.736], [320.0, 485.3593], [318.0135, 279.4593], [314.6762, 328.5291]]\n","40\n","[[0.4987062, 0.563651], [0.5, 0.5525945], [0.5382264, 0.5333939999999999], [0.4899627, 0.5360328999999999], [0.5085652, 0.3501746], [0.4936557, 0.42185229999999996], [0.5085613, 0.3501674], [0.4881113, 0.6383890999999999], [0.5, 0.5525804999999999], [0.5179487, 0.056047599999999975], [0.4945675, 0.4230996], [0.5148086, 0.12296450000000003], [0.5, 0.703283], [0.5064632, 0.33616650000000003], [0.483354, 0.5286341], [0.5019736, 0.5184847], [0.5200818, 0.4917138], [0.5094502, 0.4245869], [0.4976398, 0.5230427], [0.5015073, 0.5329699], [0.5019613, 0.44938769999999995], [0.4991458, 0.4703159], [0.499146, 0.3768104], [0.5018758, 0.4492454], [0.5019183, 0.4493161], [0.4943362, 0.4793417], [0.4940712, 0.4821516], [0.5014035, 0.6505963], [0.5014858, 0.41379449999999995], [0.4936377, 0.48229120000000003], [0.5013987, 0.5366961], [0.5012838, 0.5364107], [0.5012822, 0.5364124], [0.5010208, 0.6041888], [0.5011398, 0.5360265], [0.5, 0.6081664], [0.5, 0.53695], [0.5, 1.01116514], [0.4968961, 0.5822069000000001], [0.4916815, 0.6844356]]\n","40\n"]}],"source":["# 원본 좌표값과 실수형 좌표값을 불러옴\n","origin_coordinate = []\n","float_origin_coordinate = []\n","\n","# 원점 좌표값 불러오기\n","for label_file in os.listdir(lables):\n","    label_path = os.path.join(lables, label_file)\n","    with open(label_path, \"r\") as f:\n","        json_file = json.load(f)\n","        coord = json_file[\"targetAnnotation\"]\n","        float_coord = copy.deepcopy(coord)\n","        float_origin_coordinate.append(float_coord)\n","        \n","        coord[0] = round(coord[0] * 640, 4)\n","        coord[1] = round(coord[1] * 480, 4)\n","        origin_coordinate.append(coord)\n","\n","print(origin_coordinate)\n","print(len(origin_coordinate))\n","\n","print(float_origin_coordinate)\n","print(len(float_origin_coordinate))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#원본 이미지를 가져오는 코드\n","len_coord = len(origin_coordinate)\n","\n","target_images = []\n","for image_file in os.listdir(target_image_dir):\n","    image_path = os.path.join(target_image_dir, image_file)\n","    target_images.append(image_path)\n","\n","# 에러를 저장할 리스트\n","disappear_errors = []\n","misannotate_errors = []\n","pixel_errors = []\n","\n","missing_inlier = 0\n","failed_inliers = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def matching_keypoints(target_img, video_img, stabilizing=False):\n","    # 이미지를 불러옴\n","    img0 = load_image(target_img, grayscale=True)\n","    if stabilizing == True:\n","        img1 = stabilizer.stabilize_frame(video_img)\n","        img1 = load_image(img1, grayscale=True)\n","    else:\n","        img1 = load_image(video_img , grayscale=True)\n","\n","    # extract local features\n","    feats0 = extractor.extract(img0.to(device))  # auto-resize the image, disable with resize=None\n","    feats1 = extractor.extract(img1.to(device))\n","\n","    # match the features\n","    matches01 = matcher({'image0': feats0, 'image1': feats1})\n","    feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]  # remove batch dimension\n","\n","    # get results\n","    kpts0 = feats0[\"keypoints\"]\n","    kpts1 = feats1[\"keypoints\"]\n","    matches = matches01['matches']  # indices with shape (K,2)\n","    points0 = kpts0[matches[..., 0]]  # coordinates in img0, shape (K,2)\n","    points1 = kpts1[matches[..., 1]]  # coordinates in img1, shape (K,2)\n","\n","    return {\n","        \"points0\": points0,\n","        \"points1\": points1,\n","    }\n","    \n","#에러 측정 \n","def get_errors(disappear_error, misannotate_error, pixel_error, keypoint, mask, coord, x, y):\n","    if len(keypoint) < 6:\n","        missing_inlier += 1\n","    \n","    if mask == 0.3:\n","        failed_inliers += 1\n","    \n","    _x = coord[0] / 640\n","    _y = coord[1] / 480\n","    \n","    if _x < 0 or _x > 1 or _y < 0 or _y > 1:\n","        disappear_error += 1\n","        \n","    distance = math.sqrt((x - _x)**2 + (y - _y)**2)\n","    \n","    if distance > 0.1:\n","        misannotate_error += 1\n","        \n","    if distance > pixel_error:\n","        pixel_error = distance\n","        \n","    return {\n","        \"disappear_error\": disappear_error,\n","        \"misannotate_error\": misannotate_error,\n","        \"pixel_error\": pixel_error,\n","        \"missing_inlier\": missing_inlier,\n","        \"failed_inliers\": failed_inliers\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"unmatched ')' (3967670478.py, line 34)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[11], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    homography, mask = CSRansac.csransac(target_keypoint, frame_keypoint))\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"]}],"source":["# 매칭되는 호모그래피를 구하여 원점을 투영 변환한 후, 에러를 측정\n","# 에러 측정을 위해 총 10번 반복\n","for i in range(10):\n","    disappear_error = 0\n","    misannotate_error = 0\n","    pixel_error = 0\n","    \n","    for i in range(len_coord):\n","        target_image = target_images[i]\n","        len_target_image = len(target_images)\n","        \n","        x = origin_coordinate[i][0]\n","        y = origin_coordinate[i][1]\n","        \n","        target_image = load_image(target_image, grayscale=True)\n","        \n","        # vide_dir에 있는 모든 비디오를 가져옴\n","        for video_file in os.listdir(video_dir):\n","            video_path = os.path.join(video_dir, video_file)\n","            cap = cv2.VideoCapture(video_path)\n","            \n","            # 각 프레임 처리, 에러처리도 동시에 진행\n","            while True:\n","                ret, frame = cap.read()\n","                if not ret:\n","                    break\n","                \n","                # 특징점 매칭\n","                results = matching_keypoints(target_image, frame)\n","                target_keypoint = results[\"points0\"].cpu().numpy()\n","                frame_keypoint = results[\"points1\"].cpu().numpy()\n","                \n","               # 호모그래피 추정\n","                homography, mask = CSRansac.csransac(target_keypoint, frame_keypoint))\n","                projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","                \n","                # 에러 측정\n","                error_results = get_errors(disappear_error, misannotate_error, pixel_error, target_keypoint, mask, projected_pts, x, y)\n","                    \n","            cap.release()\n","\n","    disappear_errors.append(disappear_error)\n","    misannotate_errors.append(misannotate_error)\n","    pixel_errors.append(pixel_error)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["disappear_error: 279.5\n","num_error: 867.2\n","pixel_error: 10.09973137927756\n"]}],"source":["error1 = sum(disappear_errors) / len(disappear_errors)\n","error2 = sum(misannotate_errors) / len(misannotate_errors)\n","error3 = sum(pixel_errors) / len(pixel_errors)\n","\n","print(\"disappear_error:\", error1)\n","print(\"num_error:\", error2)\n","print(\"pixel_error:\", error3)\n","print(\"missing_inlier:\", missing_inlier)\n","print(\"failed_inliers:\", failed_inliers)   "]},{"cell_type":"markdown","metadata":{},"source":["## check speed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"IndexError","evalue":"max(): Expected reduction dim 2 to have non-zero size.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m image1 \u001b[38;5;241m=\u001b[39m load_image(image1, grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m feats1 \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mextract(image1\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m---> 29\u001b[0m matches01 \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m feats0, feats1, matches01 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     32\u001b[0m     rbd(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [feats0, feats1, matches01]\n\u001b[0;32m     33\u001b[0m ]  \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n\u001b[0;32m     35\u001b[0m kpts0, kpts1, matches \u001b[38;5;241m=\u001b[39m feats0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], feats1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m], matches01[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\ailab\\anaconda3\\envs\\lightglue\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\ailab\\anaconda3\\envs\\lightglue\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\ailab\\LightGlue\\lightglue\\lightglue.py:471\u001b[0m, in \u001b[0;36mLightGlue.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03mMatch keypoints and descriptors between two images\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    matches: List[[Si x 2]], scores: List[[Si]]\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mmp, device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\ailab\\LightGlue\\lightglue\\lightglue.py:582\u001b[0m, in \u001b[0;36mLightGlue._forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    580\u001b[0m desc0, desc1 \u001b[38;5;241m=\u001b[39m desc0[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :m, :], desc1[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n, :]  \u001b[38;5;66;03m# remove padding\u001b[39;00m\n\u001b[0;32m    581\u001b[0m scores, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_assignment[i](desc0, desc1)\n\u001b[1;32m--> 582\u001b[0m m0, m1, mscores0, mscores1 \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m matches, mscores \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(b):\n","File \u001b[1;32mc:\\Users\\ailab\\LightGlue\\lightglue\\lightglue.py:298\u001b[0m, in \u001b[0;36mfilter_matches\u001b[1;34m(scores, th)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_matches\u001b[39m(scores: torch\u001b[38;5;241m.\u001b[39mTensor, th: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m    297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"obtain matches from a log assignment matrix [Bx M+1 x N+1]\"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     max0, max1 \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, scores[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    299\u001b[0m     m0, m1 \u001b[38;5;241m=\u001b[39m max0\u001b[38;5;241m.\u001b[39mindices, max1\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m    300\u001b[0m     indices0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(m0\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mm0\u001b[38;5;241m.\u001b[39mdevice)[\u001b[38;5;28;01mNone\u001b[39;00m]\n","\u001b[1;31mIndexError\u001b[0m: max(): Expected reduction dim 2 to have non-zero size."]}],"source":["# video_frames 폴더에서 프레임 파일 리스트 가져오기\n","video_frames = os.listdir('video')\n","\n","# 프레임 수 초기화\n","frame_count = 0\n","\n","# 프레임 별 처리 시간 리스트 초기화\n","frame_processing_times = []\n","\n","x = 637 // 2\n","y = 367 // 2\n","\n","\n","cap = cv2.VideoCapture('demo_video_resized.mp4')\n","\n","# 각 프레임 처리\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    start_time = time.time()\n","    \n","    image0 = load_image(\"img1.png\", grayscale=True)\n","    feats0 = extractor.extract(image0.to(device))\n","    image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(image1, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, _ = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","    \n","    # 현재 시간 측정\n","    current_time = time.time()\n","    \n","    # 프레임 처리 시간 계산\n","    frame_processing_time = current_time - start_time\n","    frame_processing_times.append(frame_processing_time)\n","    \n","    # 이전 프레임 처리 시간 업데이트\n","    prev_frame_time = current_time\n","\n","    # FPS 계산\n","    fps = 1.0 / frame_processing_time\n","\n","    # 프레임 수 증가\n","    frame_count += 1\n","\n","    \n","    cv2.imshow('frame', frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","    \n","    \n","cap.release()\n","cv2.destroyAllWindows()\n","\n","# 전체 처리 시간 계산\n","total_processing_time = sum(frame_processing_times)\n","\n","# 전체 프레임 수와 전체 처리 시간을 사용하여 평균 FPS 계산\n","average_fps = frame_count / total_processing_time\n","\n","print(f\"Total Frames Processed: {frame_count}\")\n","print(f\"Average FPS: {average_fps:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from vidstab import VidStab\n","\n","# Using defaults\n","stabilizer = VidStab()\n","stabilizer.stabilize(input_path='demo_video_resized.mp4', output_path='stable_demo_video.mp4')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["disappear_error: 0\n","misannotate_error: 334\n","pixel_error: 0.14165977565377297\n"]}],"source":["cap = cv2.VideoCapture('datasets/unstable_version.mp4')\n","\n","x = 319\n","y = 238\n","\n","disappear_error = 0\n","misannotate_error = 0\n","pixel_error = 0\n","\n","image0 = load_image(\"datasets/origin.png\", grayscale=True)\n","\n","fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n","out = cv2.VideoWriter('before_stabilize.avi', fourcc, 30, (640, 480))\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    \n","    feats0 = extractor.extract(image0.to(device))\n","    #image1 = stabilizer.stabilize_frame(input_frame = frame)\n","    image1 = load_image(frame, grayscale=True)\n","    feats1 = extractor.extract(image1.to(device))\n","    \n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    \n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","    \n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","    \n","    homography, mask = CSRansac.csransac(m_kpts0.cpu().numpy(), m_kpts1.cpu().numpy())\n","    if mask < 0.3:\n","        cv2.imshow('frame', frame)\n","        continue\n","    projected_pts = CSRansac.perspective_transform(np.array([x, y]), homography)\n","    \n","    _x = projected_pts[0] / 640\n","    _y = projected_pts[1] / 480\n","    \n","    # 에러 측정\n","    if _x < 0 or _x > 1 or _y < 0 or _y > 1:\n","        disappear_error += 1\n","        \n","    distance = math.sqrt((x / 640 - _x)**2 + (y / 640 - _y)**2)\n","    \n","    if distance > 0.1:\n","        misannotate_error += 1\n","        \n","    if distance > pixel_error:\n","        pixel_error = distance\n","    \n","    cv2.circle(frame, (int(projected_pts[0]), int(projected_pts[1])), 5, (0, 0, 255), -1)\n","    \n","    cv2.imshow('frame', frame)\n","    \n","    out.write(frame)\n","    \n","    key = cv2.waitKey(5)\n","    if key == 27:\n","        break\n","\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()\n","\n","print(\"disappear_error:\", disappear_error)\n","print(\"misannotate_error:\", misannotate_error)\n","print(\"pixel_error:\", pixel_error)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
